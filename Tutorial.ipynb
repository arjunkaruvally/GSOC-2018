{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSOC 2018 - Reverse Transition Dynamics of Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to understand the reverse transitional dynamics of a stochastic process conditioned on the terminal observation. In this tutorial we check how different samplers perform in this task. For the purpose of the tutorial we take a simple stochastic process - One Dimensional Random Walk. For the demonstration, we use the Monte Carlo Sampler(MCSampler), Importance Sampling Samplers(ISSamplers) with a Funnel proposal, ISSampler with a softer version of the Funnel proposal and finally a reinforcement learning approach - reinforced variational inference(RVI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: make sure the repository is setup following directions from https://github.com/zafarali/better-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import some important packages\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from rvi_sampling import utils\n",
    "from rvi_sampling.samplers import ISSampler, ABCSampler, MCSampler\n",
    "from rvi_sampling.distributions.proposal_distributions import FunnelProposal, SimonsSoftProposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the parametersof the random walk process. To simplify, a one dimensional random walk is taken and the process is unbiased(means that each step the direction the walk takes is chosen randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIMENSIONS = 1    # Set dimension of the random walk\n",
    "OUTPUT_SIZE = 2   # The output dimension of sampler networks (action, action probabilities)\n",
    "BIASED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Command line parsers can be created using the `utils.parsers.create_parser` function. This adds basic command line arguments for rvi sampling and basic experimental arguments\n",
    "```\n",
    "parser = utils.parsers.create_parser('1D random walk', 'random_walk')\n",
    "```\n",
    "\n",
    "\n",
    "additional required arguments can be added using the `parser.add_argument` function\n",
    "```\n",
    "parser.add_argument('-cycles', '--cycles', type=int, default=15,\n",
    "                    help='number of train-test cycles.')\n",
    "```\n",
    "\n",
    "The `parser.parse_args` function execute the parser on the command line arguments and we get the parameters in the variable assigned to it.\n",
    "```\n",
    "\n",
    "args = parser.parse_args()\n",
    "```\n",
    "\n",
    "#### NOTE: For the purpose of tutorial, the args variable is set manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### command line arguments\n",
    "\n",
    "class Arguments():\n",
    "    def __init__(\n",
    "        self,\n",
    "        entropy = 0,  # Rvi environment\n",
    "        baseline_decay = 0.99,\n",
    "        learning_rate = 0.001,\n",
    "        baseline_learning_rate = 0.001,\n",
    "        only_rvi = False,\n",
    "        no_train = False,\n",
    "        baseline_type = 'moving_average',\n",
    "        notime = True,\n",
    "        gamma = 1,\n",
    "        rewardclip = -10,\n",
    "        gae = False,\n",
    "        lam = 1.0,\n",
    "        n_agents = 1,\n",
    "        plot_posterior = False,\n",
    "        neural_network = [16, 16],\n",
    "        pretrained = None,\n",
    "        samples = 1000, # experimental arguments\n",
    "        sampler_seed = 0,\n",
    "        method = \"ISSampler\",\n",
    "        n_cpus = 3,\n",
    "        no_tensorboard = False,\n",
    "        name = 'results',\n",
    "        IS_proposal = 'funnel',\n",
    "        softness_coefficient = 1.0,\n",
    "        override_endpoint = False,\n",
    "        outfolder = './',\n",
    "        profile_performance = False\n",
    "    ):\n",
    "        self.entropy = entropy\n",
    "        self.baseline_decay = baseline_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.baseline_learning_rate = baseline_learning_rate\n",
    "        self.only_rvi = only_rvi\n",
    "        self.no_train = no_train\n",
    "        self.baseline_type = baseline_type\n",
    "        self.notime = notime\n",
    "        self.gamma = gamma\n",
    "        self.rewardclip = rewardclip\n",
    "        self.gae = gae\n",
    "        self.lam = lam\n",
    "        self.n_agents = n_agents,\n",
    "        self.plot_posterior = plot_posterior,\n",
    "        self.neural_network = neural_network,\n",
    "        self.pretrained = pretrained,\n",
    "        self.samples = samples, # experimental arguments\n",
    "        self.sampler_seed = sampler_seed,\n",
    "        self.method = method,\n",
    "        self.n_cpus = n_cpus,\n",
    "        self.no_tensorboard = no_tensorboard,\n",
    "        self.name = name,\n",
    "        self.IS_proposal = IS_proposal,\n",
    "        self.softness_coefficient = softness_coefficient,\n",
    "        self.override_endpoint = override_endpoint,\n",
    "        self.outfolder = outfolder,\n",
    "        self.profile_performance = profile_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set some of the required aspects of the experiments - seeds(for reproducibility), folders to save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets the global seed for the random number generators\n",
    "utils.common.set_global_seeds(args.sampler_seed)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create the folder name for where the results are to be stored\n",
    "folder_name = utils.io.create_folder_name(args.outfolder, args.name+'_'+str(args.sampler_seed)+'_'+str(args.rw_seed)+'_'+str(args.method))\n",
    "\n",
    "# Training results are stored in separate train folder\n",
    "train_folder_name = os.path.join(folder_name, 'training_results')\n",
    "\n",
    "train_folder_to_save_in = os.path.join(train_folder_name, '0')\n",
    "utils.io.create_folder(train_folder_to_save_in)\n",
    "\n",
    "# This tracks the training kl divergence results cumulatively\n",
    "kl_train_cumulative_track = os.path.join(folder_name, 'kl_training_cumulative.txt')\n",
    "kl_train_track = os.path.join(folder_name, 'kl_training.txt')\n",
    "\n",
    "# This trackes the proposal success rates cumulatively\n",
    "prop_train_cumulative_track = os.path.join(folder_name, 'prop_training_cumulative.txt')\n",
    "prop_train_track = os.path.join(folder_name, 'prop_training.txt')\n",
    "\n",
    "# These functions create the folders required for saving results\n",
    "utils.io.create_folder(folder_name)\n",
    "utils.io.create_folder(train_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a random walk process-it is created using the `utils.stochastic_process.create_rw(<args>, biased=<True/False>, n_agents=<number of agents interacting with the process>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function creates the random walk with the given parameters\n",
    "# The n_agents parameter shows how many agents are interacting with the random walk\n",
    "# Different stochastic processes can be implemented similar to random walk\n",
    "rw, analytic = utils.stochastic_processes.create_rw(args, biased=BIASED, n_agents=args.n_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The override endpoint command line argument will help to create different random processes - For example a simple random walk process can be made difficult to sample by making the endpoint farther from the starting window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This argument decides if we want to override the endpoint of the random walk process\n",
    "if args.override_endpoint:\n",
    "    rw.xT = np.array([ args.endpoint ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.io.touch(os.path.join(folder_name, 'start={}'.format(rw.x0)))\n",
    "utils.io.touch(os.path.join(folder_name, 'end={}'.format(rw.xT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this argument sets where the ISproposal should push toward\n",
    "push_toward = [-args.rw_width, args.rw_width]\n",
    "\n",
    "# The soft proposal makes IS proposal softer such that the push towards is lighter\n",
    "# the intensity of softness is given by the softness coefficient\n",
    "if args.IS_proposal == 'soft':\n",
    "    proposal = SimonsSoftProposal(push_toward, softness_coeff=args.softness_coefficient)\n",
    "else:\n",
    "    proposal = FunnelProposal(push_toward)\n",
    "\n",
    "if args.method == 'ISSampler':\n",
    "    sampler = ISSampler(proposal, seed=args.sampler_seed)\n",
    "elif args.method == 'MCSampler':\n",
    "    sampler = MCSampler(seed=args.sampler_seed)\n",
    "elif args.method == 'ABCSampler':\n",
    "    sampler = ABCSampler('slacked',seed=args.sampler_seed)\n",
    "else:\n",
    "    raise ValueError('Unknown method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_function(estimated_distribution):\n",
    "    return analytic.kl_divergence(estimated_distribution, rw.xT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a diagnostic can be used to track the different samplers at different training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler.set_diagnostic(utils.diagnostics.create_diagnostic(sampler._name, args, folder_name, kl_function))\n",
    "\n",
    "print('True Starting Position is:{}'.format(rw.x0))\n",
    "print('True Ending Position is: {}'.format(rw.xT))\n",
    "print('Analytic Starting Position: {}'.format(analytic.expectation(rw.xT[0])))\n",
    "\n",
    "train_results = None\n",
    "\n",
    "utils.io.touch(kl_train_track)\n",
    "utils.io.touch(kl_train_cumulative_track)\n",
    "utils.io.touch(prop_train_track)\n",
    "utils.io.touch(prop_train_cumulative_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the actual experiment is done - to make diagnostic easier, the sampler is run `args.cycles` number of times. Each cycle contains `args.samples` number of mc steps. At the end of each cycle, the kl divergences and proposal success rates are saved in files in the experiment folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, args.cycles+1):\n",
    "    train_results_new = sampler.solve(rw, args.samples)\n",
    "\n",
    "    # technically doing this saving doesn't take too long so doesn't need to be run\n",
    "    # in a background thread. This is good because it saves time of having to copy\n",
    "    # the policy for saving etc.\n",
    "    if train_results is None:\n",
    "        train_results = train_results_new\n",
    "    else:\n",
    "        # augment the old Results object.\n",
    "        train_results._all_trajectories.extend(train_results_new.all_trajectories())\n",
    "        train_results._trajectories.extend(train_results_new.trajectories())\n",
    "        train_results._posterior_particles = np.hstack([train_results.posterior(),\n",
    "                                                        train_results_new.posterior()])\n",
    "\n",
    "        train_results._posterior_weights = np.hstack([train_results.posterior_weights(),\n",
    "                                                      train_results_new.posterior_weights()])\n",
    "\n",
    "\n",
    "    steps_so_far = str(i * args.samples)\n",
    "\n",
    "\n",
    "    train_folder_to_save_in = os.path.join(train_folder_name, str(i))\n",
    "    utils.io.create_folder(train_folder_to_save_in)\n",
    "    print('Training Phase:')\n",
    "    kld = utils.analysis.analyze_samplers_rw([train_results], args, None, rw,\n",
    "                                       policy=None, analytic=analytic) # don't save these things again\n",
    "\n",
    "    utils.io.stash(kl_train_cumulative_track, steps_so_far + ', ' + str(kld[0]))\n",
    "    utils.io.stash(prop_train_cumulative_track, steps_so_far + ', ' + str(train_results.prop_success()))\n",
    "\n",
    "\n",
    "    kld = utils.analysis.analyze_samplers_rw([train_results_new], args, train_folder_to_save_in, rw,\n",
    "                                       policy=None, analytic=analytic) # don't save these things again\n",
    "    utils.io.stash(kl_train_track, steps_so_far + ', ' + str(kld[0]))\n",
    "    utils.io.stash(prop_train_track, steps_so_far + ', ' + str(train_results_new.prop_success()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at how different samplers behave in different scenarios - we look at a 2 different settings of simple random walk \n",
    "\n",
    "* the start window and endpoint are nearer(endpoint 0). \n",
    "* Then we look at a more difficult setting where the random walk endpoint is farther from the starting window(endpoint 8). The second setting is more difficult because the sampler has to take explore low probabilty trajectories.\n",
    "\n",
    "![alt text](img/stochastic_process.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampler\n",
    "\n",
    "Monte Carlo Sampler takes a random direction at each step of the random walk. For random walks with endpoints near the start region MC Sampler works well.\n",
    "\n",
    "We look at some sample trajectories below\n",
    "\n",
    "#### endpoint 0\n",
    "\n",
    "![alt text](img/successful_trajectories_mc_end0.jpg)\n",
    "\n",
    "#### endpoint 8\n",
    "\n",
    "![no_img](img/successful_trajectories_mc_end8.jpg)\n",
    "\n",
    "As it can be noticed, Monte Carlo sampler works very poorly for the more difficult setting because it fails to capture the low probability trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling Sampler\n",
    "\n",
    "The importance sampling sampler uses a Funnel proposal.\n",
    "\n",
    "## Funnel proposal\n",
    "\n",
    "![no_img](img/funnel_proposal.jpg)\n",
    "\n",
    "Some sample trajectories are shown below\n",
    "\n",
    "#### endpoint 0\n",
    "\n",
    "![no_img](img/successful_trajectories_is_end0.jpg)\n",
    "\n",
    "#### endpoint 8\n",
    "\n",
    "![no_img](img/successful_trajectories_is_end8.jpg)\n",
    "\n",
    "It can be noticed that for the importance sampling method, we get more successful trajectories from both the settings due to the Funnnel proposal which pushes the trajectory in the required direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Sampling with Soft Proposal\n",
    "\n",
    "Here we use a softer proposal. The funnel proposal is a bit strict when it comes to trajectories in the sense that once a trajectory hits the boundary, the proposal pushes it to the endpoint. This results in having many trajectories starting up in the edge of the window. The Soft proposal gets over this limitation by only having softer push towards the window region. This results in a distribution which is much softer and resembles more with the target distribution.\n",
    "\n",
    "#### endpoint 0\n",
    "\n",
    "![no_img](img/successful_trajectories_issoft_end0.jpg)\n",
    "\n",
    "#### endpoint 8\n",
    "\n",
    "![no_img](img/successful_trajectories_issoft_end8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RVI Sampler\n",
    "\n",
    "There has been some theoretical work on the use of Reinforcement Learning techniques for Variational Inference. In the RVI sampler some of these theoretical evidences is applied in practice to understand the practical effectiveness of the method. The samples shown below are from a reinforcement learning method with 1 agent, and using a variance reduction technique called Generalized Advantage Estimation with lambda 0.95.\n",
    "\n",
    "#### endpoint 0\n",
    "\n",
    "![no_img](img/rvi_success_traj_0.jpg)\n",
    "\n",
    "The image shows how the RVI Sampler performs at different points in the training process. It can be noticed that more trajectories are successful in the final steps in training.\n",
    "\n",
    "#### endpoint  8\n",
    "\n",
    "![no_img](img/rvi_success_traj_8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Now we look at how different samplers(Monte Carlo, Importance sampling with Funnel proposal, Importance Sampling with a soft proposal, Reinforced Variational Inference) behave in different conditions. The endpoint of the process is changed to reflect different difficulty conditions. Endpoints farther from the starting position requires low probability trajectories to be successful. Monte Carlo sampler performs poorly in these adverse conditions but the performance of the IS sampler with soft proposal is not effected much by the changing difficulty.\n",
    "\n",
    "![no_img](img/difficulty_comparissons.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Commits\n",
    "\n",
    "- Profiling - https://github.com/zafarali/better-sampling/commit/8148b3027d505032f2b76f5d3f4903b31d14466b\n",
    "- Pytorch Version upgrade - https://github.com/zafarali/better-sampling/commit/8148b3027d505032f2b76f5d3f4903b31d14466b ; https://github.com/zafarali/policy-gradient-methods/commit/8bea455982ab7f0768951f467c5a96c7038be6ee\n",
    "- Enhanced tests - https://github.com/zafarali/better-sampling/commit/5a7c6e369406b1b92ed8d914d89171bd1edf4d2f\n",
    "- Function approximation - https://github.com/zafarali/better-sampling/commits/fn_approximator_baseline\n",
    "- GAE implementation - https://github.com/zafarali/policy-gradient-methods/commit/52fb37694dc94e74f62d83f39dfb9597394b6de3\n",
    "- Varying difficulty experiments - https://github.com/zafarali/better-sampling/commits/varying_difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acnowledgements\n",
    "\n",
    "I thank Dr. Simon Gravel and Zafarali Ahmed for being patient and helping out in the various aspects of the project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rvi)",
   "language": "python",
   "name": "rvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
